{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as BS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Request https://realpython.github.io/fake-jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://realpython.github.io/fake-jobs'\n",
    "response = requests.get(URL)\n",
    "response.status_code\n",
    "soup = BS(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the tag containing the first job title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Senior Python Developer'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find('h2').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now extract **all** job titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Senior Python Developer',\n",
       " 'Energy engineer',\n",
       " 'Legal executive',\n",
       " 'Fitness centre manager',\n",
       " 'Product manager',\n",
       " 'Medical technical officer',\n",
       " 'Physiological scientist',\n",
       " 'Textile designer',\n",
       " 'Television floor manager',\n",
       " 'Waste management officer',\n",
       " 'Software Engineer (Python)',\n",
       " 'Interpreter',\n",
       " 'Architect',\n",
       " 'Meteorologist',\n",
       " 'Audiological scientist',\n",
       " 'English as a second language teacher',\n",
       " 'Surgeon',\n",
       " 'Equities trader',\n",
       " 'Newspaper journalist',\n",
       " 'Materials engineer',\n",
       " 'Python Programmer (Entry-Level)',\n",
       " 'Product/process development scientist',\n",
       " 'Scientist, research (maths)',\n",
       " 'Ecologist',\n",
       " 'Materials engineer',\n",
       " 'Historic buildings inspector/conservation officer',\n",
       " 'Data scientist',\n",
       " 'Psychiatrist',\n",
       " 'Structural engineer',\n",
       " 'Immigration officer',\n",
       " 'Python Programmer (Entry-Level)',\n",
       " 'Neurosurgeon',\n",
       " 'Broadcast engineer',\n",
       " 'Make',\n",
       " 'Nurse, adult',\n",
       " 'Air broker',\n",
       " 'Editor, film/video',\n",
       " 'Production assistant, radio',\n",
       " 'Engineer, communications',\n",
       " 'Sales executive',\n",
       " 'Software Developer (Python)',\n",
       " 'Futures trader',\n",
       " 'Tour manager',\n",
       " 'Cytogeneticist',\n",
       " 'Designer, multimedia',\n",
       " 'Trade union research officer',\n",
       " 'Chemist, analytical',\n",
       " 'Programmer, multimedia',\n",
       " 'Engineer, broadcasting (operations)',\n",
       " 'Teacher, primary school',\n",
       " 'Python Developer',\n",
       " 'Manufacturing systems engineer',\n",
       " 'Producer, television/film/video',\n",
       " 'Scientist, forensic',\n",
       " 'Bonds trader',\n",
       " 'Editorial assistant',\n",
       " 'Photographer',\n",
       " 'Retail banker',\n",
       " 'Jewellery designer',\n",
       " 'Ophthalmologist',\n",
       " 'Back-End Web Developer (Python, Django)',\n",
       " 'Licensed conveyancer',\n",
       " 'Futures trader',\n",
       " 'Counselling psychologist',\n",
       " 'Insurance underwriter',\n",
       " 'Engineer, automotive',\n",
       " 'Producer, radio',\n",
       " 'Dispensing optician',\n",
       " 'Designer, fashion/clothing',\n",
       " 'Chartered loss adjuster',\n",
       " 'Back-End Web Developer (Python, Django)',\n",
       " 'Forest/woodland manager',\n",
       " 'Clinical cytogeneticist',\n",
       " 'Print production planner',\n",
       " 'Systems developer',\n",
       " 'Graphic designer',\n",
       " 'Writer',\n",
       " 'Field seismologist',\n",
       " 'Chief Strategy Officer',\n",
       " 'Air cabin crew',\n",
       " 'Python Programmer (Entry-Level)',\n",
       " 'Warden/ranger',\n",
       " 'Sports therapist',\n",
       " 'Arts development officer',\n",
       " 'Printmaker',\n",
       " 'Health and safety adviser',\n",
       " 'Manufacturing systems engineer',\n",
       " 'Programmer, applications',\n",
       " 'Medical physicist',\n",
       " 'Media planner',\n",
       " 'Software Developer (Python)',\n",
       " 'Surveyor, land/geomatics',\n",
       " 'Legal executive',\n",
       " 'Librarian, academic',\n",
       " 'Barrister',\n",
       " 'Museum/gallery exhibitions officer',\n",
       " 'Radiographer, diagnostic',\n",
       " 'Database administrator',\n",
       " 'Furniture designer',\n",
       " 'Ship broker']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pull all titles\n",
    "all_titles = soup.findAll('h2')\n",
    "\n",
    "# Create list of all titles as text\n",
    "job_title = [] #Don't know why I can't use a list comprehension for this - it returns all <none>...\n",
    "for x in all_titles:\n",
    "    job_title.append(x.text)\n",
    "job_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get companies, locations, and posting dates for each job into lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Payne, Roberts and Davis',\n",
       " 'Vasquez-Davidson',\n",
       " 'Jackson, Chambers and Levy',\n",
       " 'Savage-Bradley',\n",
       " 'Ramirez Inc',\n",
       " 'Rogers-Yates',\n",
       " 'Kramer-Klein',\n",
       " 'Meyers-Johnson',\n",
       " 'Hughes-Williams',\n",
       " 'Jones, Williams and Villa',\n",
       " 'Garcia PLC',\n",
       " 'Gregory and Sons',\n",
       " 'Clark, Garcia and Sosa',\n",
       " 'Bush PLC',\n",
       " 'Salazar-Meyers',\n",
       " 'Parker, Murphy and Brooks',\n",
       " 'Cruz-Brown',\n",
       " 'Macdonald-Ferguson',\n",
       " 'Williams, Peterson and Rojas',\n",
       " 'Smith and Sons',\n",
       " 'Moss, Duncan and Allen',\n",
       " 'Gomez-Carroll',\n",
       " 'Manning, Welch and Herring',\n",
       " 'Lee, Gutierrez and Brown',\n",
       " 'Davis, Serrano and Cook',\n",
       " 'Smith LLC',\n",
       " 'Thomas Group',\n",
       " 'Silva-King',\n",
       " 'Pierce-Long',\n",
       " 'Walker-Simpson',\n",
       " 'Cooper and Sons',\n",
       " 'Donovan, Gonzalez and Figueroa',\n",
       " 'Morgan, Butler and Bennett',\n",
       " 'Snyder-Lee',\n",
       " 'Harris PLC',\n",
       " 'Washington PLC',\n",
       " 'Brown, Price and Campbell',\n",
       " 'Mcgee PLC',\n",
       " 'Dixon Inc',\n",
       " 'Thompson, Sheppard and Ward',\n",
       " 'Adams-Brewer',\n",
       " 'Schneider-Brady',\n",
       " 'Gonzales-Frank',\n",
       " 'Smith-Wong',\n",
       " 'Pierce-Herrera',\n",
       " 'Aguilar, Rivera and Quinn',\n",
       " 'Lowe, Barnes and Thomas',\n",
       " 'Lewis, Gonzalez and Vasquez',\n",
       " 'Taylor PLC',\n",
       " 'Oliver, Jones and Ramirez',\n",
       " 'Rivera and Sons',\n",
       " 'Garcia PLC',\n",
       " 'Johnson, Wells and Kramer',\n",
       " 'Gonzalez LLC',\n",
       " 'Morgan, White and Macdonald',\n",
       " 'Robinson-Fitzpatrick',\n",
       " 'Waters, Wilson and Hoover',\n",
       " 'Hill LLC',\n",
       " 'Li-Gregory',\n",
       " 'Fisher, Ryan and Coleman',\n",
       " 'Stewart-Alexander',\n",
       " 'Abbott and Sons',\n",
       " 'Bryant, Santana and Davenport',\n",
       " 'Smith PLC',\n",
       " 'Patterson-Singh',\n",
       " 'Martinez-Berry',\n",
       " 'May, Taylor and Fisher',\n",
       " 'Bailey, Owen and Thompson',\n",
       " 'Vasquez Ltd',\n",
       " 'Leblanc LLC',\n",
       " 'Jackson, Ali and Mckee',\n",
       " 'Blankenship, Knight and Powell',\n",
       " 'Patton, Haynes and Jones',\n",
       " 'Wood Inc',\n",
       " 'Collins Group',\n",
       " 'Flores-Nelson',\n",
       " 'Mitchell, Jones and Olson',\n",
       " 'Howard Group',\n",
       " 'Kramer-Edwards',\n",
       " 'Berry-Houston',\n",
       " 'Mathews Inc',\n",
       " 'Riley-Johnson',\n",
       " 'Spencer and Sons',\n",
       " 'Camacho-Sanchez',\n",
       " 'Oliver and Sons',\n",
       " 'Eaton PLC',\n",
       " 'Stanley-Frederick',\n",
       " 'Bradley LLC',\n",
       " 'Parker, Goodwin and Zavala',\n",
       " 'Kim-Miles',\n",
       " 'Moreno-Rodriguez',\n",
       " 'Brown-Ortiz',\n",
       " 'Hartman PLC',\n",
       " 'Brooks Inc',\n",
       " 'Washington-Castillo',\n",
       " 'Nguyen, Yoder and Petty',\n",
       " 'Holder LLC',\n",
       " 'Yates-Ferguson',\n",
       " 'Ortega-Lawrence',\n",
       " 'Fuentes, Walls and Castro']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_companies = soup.findAll('h3')\n",
    "\n",
    "company = []\n",
    "for x in all_companies:\n",
    "    company.append(x.text)\n",
    "company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Stewartbury, AA',\n",
       " 'Christopherville, AA',\n",
       " 'Port Ericaburgh, AA',\n",
       " 'East Seanview, AP',\n",
       " 'North Jamieview, AP',\n",
       " 'Davidville, AP',\n",
       " 'South Christopher, AE',\n",
       " 'Port Jonathan, AE',\n",
       " 'Osbornetown, AE',\n",
       " 'Scotttown, AP',\n",
       " 'Ericberg, AE',\n",
       " 'Ramireztown, AE',\n",
       " 'Figueroaview, AA',\n",
       " 'Kelseystad, AA',\n",
       " 'Williamsburgh, AE',\n",
       " 'Mitchellburgh, AE',\n",
       " 'West Jessicabury, AA',\n",
       " 'Maloneshire, AE',\n",
       " 'Johnsonton, AA',\n",
       " 'South Davidtown, AP',\n",
       " 'Port Sara, AE',\n",
       " 'Marktown, AA',\n",
       " 'Laurenland, AE',\n",
       " 'Lauraton, AP',\n",
       " 'South Tammyberg, AP',\n",
       " 'North Brandonville, AP',\n",
       " 'Port Robertfurt, AA',\n",
       " 'Burnettbury, AE',\n",
       " 'Herbertside, AA',\n",
       " 'Christopherport, AP',\n",
       " 'West Victor, AE',\n",
       " 'Port Aaron, AP',\n",
       " 'Loribury, AA',\n",
       " 'Angelastad, AP',\n",
       " 'Larrytown, AE',\n",
       " 'West Colin, AP',\n",
       " 'West Stephanie, AP',\n",
       " 'Laurentown, AP',\n",
       " 'Wrightberg, AP',\n",
       " 'Alberttown, AE',\n",
       " 'Brockburgh, AE',\n",
       " 'North Jason, AE',\n",
       " 'Arnoldhaven, AE',\n",
       " 'Lake Destiny, AP',\n",
       " 'South Timothyburgh, AP',\n",
       " 'New Jimmyton, AE',\n",
       " 'New Lucasbury, AP',\n",
       " 'Port Cory, AE',\n",
       " 'Gileston, AA',\n",
       " 'Cindyshire, AA',\n",
       " 'East Michaelfort, AA',\n",
       " 'Joybury, AE',\n",
       " 'Emmatown, AE',\n",
       " 'Colehaven, AP',\n",
       " 'Port Coryton, AE',\n",
       " 'Amyborough, AA',\n",
       " 'Reynoldsville, AA',\n",
       " 'Port Billy, AP',\n",
       " 'Adamburgh, AA',\n",
       " 'Wilsonmouth, AA',\n",
       " 'South Kimberly, AA',\n",
       " 'Benjaminland, AP',\n",
       " 'Zacharyport, AA',\n",
       " 'Port Devonville, AE',\n",
       " 'East Thomas, AE',\n",
       " 'New Jeffrey, AP',\n",
       " 'Davidside, AA',\n",
       " 'Jamesville, AA',\n",
       " 'New Kelly, AP',\n",
       " 'Lake Antonio, AA',\n",
       " 'New Elizabethside, AA',\n",
       " 'Millsbury, AE',\n",
       " 'Lloydton, AP',\n",
       " 'Port Jeremy, AA',\n",
       " 'New Elizabethtown, AA',\n",
       " 'Charlesstad, AE',\n",
       " 'Josephbury, AE',\n",
       " 'Seanfurt, AA',\n",
       " 'Williambury, AA',\n",
       " 'South Jorgeside, AP',\n",
       " 'Robertborough, AP',\n",
       " 'South Saratown, AP',\n",
       " 'Hullview, AA',\n",
       " 'Philipland, AP',\n",
       " 'North Patty, AE',\n",
       " 'North Stephen, AE',\n",
       " 'Stevensland, AP',\n",
       " 'Reyesstad, AE',\n",
       " 'Bellberg, AP',\n",
       " 'North Johnland, AE',\n",
       " 'Martinezburgh, AE',\n",
       " 'Joshuatown, AE',\n",
       " 'West Ericstad, AA',\n",
       " 'Tuckertown, AE',\n",
       " 'Perezton, AE',\n",
       " 'Lake Abigail, AE',\n",
       " 'Jacobshire, AP',\n",
       " 'Port Susan, AE',\n",
       " 'North Tiffany, AA',\n",
       " 'Michelleville, AP']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_locations = soup.findAll('p')\n",
    "# There are two <p> tags showing up here, so the list should only inlucde every other entry. (Also need to strip out white space)\n",
    "\n",
    "location = []\n",
    "for index in range(1, len(all_locations), 2):\n",
    "     location.append(all_locations[index].text.strip())\n",
    "\n",
    "location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08',\n",
       " '2021-04-08']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_postings = soup.findAll('time')\n",
    "\n",
    "post_date = []\n",
    "for x in all_postings:\n",
    "    post_date.append(x.text)\n",
    "post_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m jobs \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({[job_title], [company], [location], [post_date]})\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "jobs = pd.DataFrame({job_title, company, location, post_date})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
